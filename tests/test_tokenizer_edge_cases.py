
import pytest
from app.analyzer import JapaneseTokenizer, ChineseTokenizer

class TestTokenizerEdgeCases:
    
    @pytest.fixture(autouse=True)
    def setup(self):
        self.ja_tokenizer = JapaneseTokenizer()
        self.zh_tokenizer = ChineseTokenizer()

    def test_ja_empty_string(self):
        tokens = self.ja_tokenizer.tokenize("")
        assert tokens == []

    def test_ja_whitespace_only(self):
        tokens = self.ja_tokenizer.tokenize("   \n  \t ")
        assert tokens == []

    def test_ja_punctuation_only(self):
        # Should not crash, might return empty depending on filter logic
        # Current logic filters "Auxiliary Symbol" etc.
        tokens = self.ja_tokenizer.tokenize("ã€‚ã€‚ã€‚ï¼ï¼Ÿ") 
        # Verify it returns a list (empty or not)
        assert isinstance(tokens, list)

    def test_ja_mixed_script(self):
        # English mixed with Japanese
        text = "Hello ã“ã‚“ã«ã¡ã¯ World"
        tokens = self.ja_tokenizer.tokenize(text)
        # Should contain "ã“ã‚“ã«ã¡ã¯" parts
        lemmas = [t[0] for t in tokens]
        # "ã“ã‚“ã«ã¡ã¯" is often normalized to "ä»Šæ—¥ã¯" by some dictionaries/tokenizers
        assert "ã“ã‚“ã«ã¡ã¯" in lemmas or "ä»Šæ—¥ã¯" in lemmas
        
    def test_ja_emoji(self):
        text = "Hello ğŸ˜º World"
        tokens = self.ja_tokenizer.tokenize(text)
        assert isinstance(tokens, list)
        
    def test_zh_empty_string(self):
        tokens = self.zh_tokenizer.tokenize("")
        assert tokens == []

    def test_zh_mixed_script(self):
        # Latin mixed with Chinese
        text = "Hello ä½ å¥½ World"
        tokens = self.zh_tokenizer.tokenize(text)
        lemmas = [t[0] for t in tokens]
        assert "ä½ å¥½" in lemmas
        
    def test_zh_punctuation(self):
        text = "ä½ å¥½ï¼Œä¸–ç•Œã€‚"
        tokens = self.zh_tokenizer.tokenize(text)
        lemmas = [t[0] for t in tokens]
        # Punctuation might be filtered
        assert "ä½ å¥½" in lemmas
        assert "ä¸–ç•Œ" in lemmas
